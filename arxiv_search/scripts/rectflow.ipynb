{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbc1e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arxiv_search import load_model\n",
    "conditioning_model = load_model(\"/home/akalmbach/arxiv_crawler/arxiv_search/models/model_500.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326ed553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional, Type\n",
    "from timm.models.vision_transformer import Attention\n",
    "import math\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_features: int,\n",
    "            out_features: Optional[int] = None,\n",
    "            hidden_features: Optional[int] = None,\n",
    "            act_layer: Type[nn.Module] = nn.GELU,\n",
    "            norm_layer: Optional[Type[nn.Module]] = None,\n",
    "            drop: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.drop1 = nn.Dropout(drop)\n",
    "        self.norm = nn.LayerNorm(hidden_features) if norm_layer is not None else nn.Identity()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop2 = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x\n",
    "\n",
    "class DiTBlock1d(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads=4, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(dim=hidden_size, num_heads=num_heads, qkv_bias=True)\n",
    "        self.norm_attn = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        self.mlp = MLP(in_features=hidden_size, hidden_features=int(hidden_size * mlp_ratio))\n",
    "        self.norm_mlp = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        self.adaln = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size, hidden_size * 6, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        print(x.shape, c.shape)\n",
    "        shift_attn, scale_attn, gate_attn, shift_mlp, scale_mlp, gate_mlp = self.adaln(c).chunk(6, dim=1)\n",
    "\n",
    "        print(shift_attn.shape, scale_attn.shape, gate_attn.shape, shift_mlp.shape, scale_mlp.shape, gate_mlp.shape)\n",
    "\n",
    "        attn_out = self.attn(\n",
    "            self.norm_attn(\n",
    "                shift_attn[:, None] + (scale_attn[:, None] + 1) * x\n",
    "            )\n",
    "        )\n",
    "\n",
    "        mlp_out = self.mlp(\n",
    "            self.norm_mlp(\n",
    "                shift_mlp[:, None] + (scale_mlp[:, None] + 1) * x\n",
    "            )\n",
    "        )\n",
    "\n",
    "        x = x + gate_attn[:, None] * attn_out + gate_mlp[:, None] * mlp_out\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads=4, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(dim=hidden_size, num_heads=num_heads, qkv_bias=True) # Regular self-attention\n",
    "        self.norm_attn = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        self.mlp = MLP(in_features=hidden_size, hidden_features=int(hidden_size * mlp_ratio))\n",
    "        self.norm_mlp = nn.LayerNorm(hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, c):\n",
    "\n",
    "        attn_out = self.attn(\n",
    "            torch.cat([self.norm_attn(x), c], dim=1)\n",
    "        )\n",
    "        mlp_out = self.mlp(self.norm_mlp(x))\n",
    "\n",
    "        x = x + attn_out[:, :x.shape[1]] + mlp_out\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TimestepEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    Embeds scalar timesteps into vector representations.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, frequency_embedding_size=256):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(frequency_embedding_size, hidden_size, bias=True),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size, hidden_size, bias=True),\n",
    "        )\n",
    "        self.frequency_embedding_size = frequency_embedding_size\n",
    "\n",
    "    @staticmethod\n",
    "    def timestep_embedding(t, dim, max_period=10000):\n",
    "        \"\"\"\n",
    "        Create sinusoidal timestep embeddings.\n",
    "        :param t: a 1-D Tensor of N indices, one per batch element.\n",
    "                          These may be fractional.\n",
    "        :param dim: the dimension of the output.\n",
    "        :param max_period: controls the minimum frequency of the embeddings.\n",
    "        :return: an (N, D) Tensor of positional embeddings.\n",
    "        \"\"\"\n",
    "        # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n",
    "        half = dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
    "        ).to(device=t.device)\n",
    "        args = t[:, None].float() * freqs[None]\n",
    "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "        if dim % 2:\n",
    "            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "        return embedding\n",
    "\n",
    "    def forward(self, t):\n",
    "        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)\n",
    "        t_emb = self.mlp(t_freq)\n",
    "        return t_emb\n",
    "\n",
    "\n",
    "class VelocityField1dCrossAttention(nn.Module):\n",
    "    def __init__(self, num_blocks, conditioning_model, num_heads=4, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        hidden_size = conditioning_model.config.hidden_size\n",
    "\n",
    "        self.time_embedder = TimestepEmbedder(hidden_size)\n",
    "        self.conditioning_model = conditioning_model\n",
    "        self.position_embeddings = conditioning_model.embeddings.position_embeddings\n",
    "        self.position_ids = conditioning_model.embeddings.position_ids\n",
    "        self.blocks = nn.ModuleList([\n",
    "            CrossAttentionBlock(hidden_size, num_heads, mlp_ratio) for _ in range(num_blocks)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(hidden_size, hidden_size)\n",
    "        self.act_out = conditioning_model.pooler.activation\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x, # batch, seq_len, hidden_size\n",
    "        t, # batch\n",
    "        y  # batch, seq_len, hidden_size\n",
    "    ):\n",
    "        _, seq_len, _ = x.shape\n",
    "        position_ids = self.position_ids[:, :seq_len]\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "\n",
    "        t_emb = self.time_embedder(t)[:, None, :] # -> batch, 1, hidden_size\n",
    "        y_emb = self.conditioning_model(inputs_embeds=y).last_hidden_state\n",
    "        c = y_emb + t_emb + position_embeddings\n",
    "\n",
    "        x = x + position_embeddings\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x, c)\n",
    "\n",
    "        return self.act_out(self.fc_out(x[:, 0]))\n",
    "\n",
    "class VelocityField1dDiT(nn.Module):\n",
    "    def __init__(self, num_blocks, conditioning_model, num_heads=4, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        hidden_size = conditioning_model.config.hidden_size\n",
    "\n",
    "        self.time_embedder = TimestepEmbedder(hidden_size)\n",
    "        self.conditioning_model = conditioning_model\n",
    "        self.position_embeddings = conditioning_model.embeddings.position_embeddings\n",
    "        self.position_ids = conditioning_model.embeddings.position_ids\n",
    "        self.blocks = nn.ModuleList([\n",
    "            DiTBlock1d(hidden_size, num_heads, mlp_ratio) for _ in range(num_blocks)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(hidden_size, hidden_size)\n",
    "        self.act_out = conditioning_model.pooler.activation\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x, # batch, seq_len, hidden_size\n",
    "        t, # batch\n",
    "        y  # batch, seq_len, hidden_size\n",
    "    ):\n",
    "        _, seq_len, _ = x.shape\n",
    "        position_ids = self.position_ids[:, :seq_len]\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "\n",
    "        t_emb = self.time_embedder(t) # -> batch, hidden_size\n",
    "        y_emb = self.conditioning_model(inputs_embeds=y).pooler_output\n",
    "        c = y_emb + t_emb\n",
    "\n",
    "        x = x + position_embeddings\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x, c)\n",
    "\n",
    "        return self.act_out(self.fc_out(x[:, 0]))\n",
    "\n",
    "velocity_model = VelocityField1dCrossAttention(1, conditioning_model, 4, 4.0).to('cuda:0')\n",
    "# velocity_model = VelocityField1dDiT(1, conditioning_model, 4, 4.0)\n",
    "\n",
    "x = torch.randn(16, 256, 768).to('cuda:0')\n",
    "t = torch.randn(16).to('cuda:0')\n",
    "y = torch.randn(16, 256, 768).to('cuda:0')\n",
    "# velocity_model1(x, t, y)\n",
    "velocity_model(x, t, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
