# Default configuration for citation embedding training and evaluation

data:
  data_dir: data
  hf_dataset_name: "akalmbach-kinsol/generative-papers-arxiv"  # Set to HuggingFace dataset name for auto-download
  citations_batch_size: 10000
  basemodel_name: "sentence-transformers/allenai-specter"
  
  # Dataset building settings
  embedding_batch_size: 5000  # Batch size for processing citation embeddings
  test_size: 0.1  # Fraction of papers to use for test set
  random_seed: 42  # Random seed for train/test split
  
  # Collation settings
  max_length: 256
  pad_to_multiple_of: 8
  pad_value: 0.0

model:
  hidden_size: 768
  num_hidden_layers: 1
  num_attention_heads: 12
  intermediate_size: 1536
  max_position_embeddings: 2048

training:
  batch_size: 256
  num_workers: 6
  num_epochs: 20
  learning_rate: 1e-3
  
  # Logging
  log_steps: 10
  save_steps: 500
  tensorboard_dir: runs

evaluation:
  batch_size: 256
  num_workers: 0
  max_batches: 10  # null = evaluate all batches

rectflow:
  # Velocity field architecture
  velocity_field_type: "DiT"  # "DiT" or "CrossAttention"
  num_blocks: 1  # Number of transformer blocks in velocity field
  num_heads: 4  # Number of attention heads
  mlp_ratio: 4.0  # MLP expansion ratio
  
  # Conditioning model
  conditioning_checkpoint: "models/model_500.pth"  # Path to conditioning model checkpoint

rectflow_training:
  batch_size: 128
  num_workers: 6
  num_epochs: 100
  learning_rate: 2e-4
  
  # Time weighting
  train_time_weight: "uniform"  # "uniform"
  train_time_distribution: "lognormal"  # "uniform" or "lognormal"
  
  # Logging
  log_steps: 10
  save_steps: 500
  tensorboard_dir: runs_rectflow
