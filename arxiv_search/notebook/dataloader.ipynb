{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f27f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class CitationEmbeddingDataset(data.Dataset):\n",
    "    def __init__(self, citations_file, paper_embeddings_file, batch_size):\n",
    "        self.citations_batch_size = batch_size\n",
    "        self.cached_shard = None\n",
    "        self.citation_embeddings = None\n",
    "        self.paper_embeddings = pl.read_parquet(paper_embeddings_file)\n",
    "        self.citations = pl.read_ndjson(citations_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.citations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        c = self.citations[index]\n",
    "\n",
    "        shard = c['index'][0] // self.citations_batch_size\n",
    "        if self.cached_shard != shard:\n",
    "            self.cached_shard = shard\n",
    "            self.citation_embeddings = pl.read_parquet(f\"citation_embeddings_{shard*self.citations_batch_size}.parquet\")\n",
    "\n",
    "        row_citation = self.citation_embeddings.filter(pl.col('reference_id') == c[0]['reference_id'])[0]\n",
    "        e_citation = torch.from_numpy(row_citation['token_embeddings'][0].to_numpy())\n",
    "\n",
    "        e_citer = self.paper_embeddings.filter(pl.col('arxiv_id') == c[0]['citer_arxiv_id'])[0]\n",
    "        e_citer = torch.from_numpy(e_citer['sentence_embedding'][0].to_numpy())\n",
    "\n",
    "        e_cited = self.paper_embeddings.filter(pl.col('arxiv_id') == c[0]['cited_arxiv_id'])[0]\n",
    "        output_embeds = torch.from_numpy(e_cited['sentence_embedding'][0].to_numpy())\n",
    "\n",
    "        inputs_emebds = torch.vstack([e_citer, e_citation])\n",
    "\n",
    "        return inputs_emebds, output_embeds\n",
    "\n",
    "ds = CitationEmbeddingDataset(\"citations.jsonl\", \"paper_embeddings.parquet\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14890c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from functools import partial\n",
    "from typing import Any, Optional\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "def _to_tensor(x):\n",
    "    return x if isinstance(x, torch.Tensor) else torch.as_tensor(x)\n",
    "\n",
    "def collate_embeddings_with_targets(\n",
    "    batch: list[tuple[torch.Tensor, Any]],\n",
    "    max_length: Optional[int] = None,\n",
    "    pad_to_multiple_of: Optional[int] = None,\n",
    "    pad_value: float = 0.0,\n",
    "    mask_dtype: torch.dtype = torch.bool,\n",
    "):\n",
    "    \"\"\"\n",
    "    batch: list of (X, y)\n",
    "      X: [seq_len, embed_dim] (float tensor or array)\n",
    "      y: target vector (same shape across samples; tensor/array/number/dict ok)\n",
    "\n",
    "    Returns:\n",
    "      {\n",
    "        \"inputs\": FloatTensor [B, L, K],\n",
    "        \"attention_mask\": Bool/Int Tensor [B, L] (True/1 = real token),\n",
    "        \"lengths\": LongTensor [B],  # pre-padding lengths\n",
    "        \"targets\": stacked y (shape depends on your dataset; typically [B, T])\n",
    "      }\n",
    "    \"\"\"\n",
    "    # Convert to tensors and apply truncation\n",
    "    Xs, Ys = [], []\n",
    "    for X, y in batch:\n",
    "        Xt = _to_tensor(X)  # [Li, K]\n",
    "        if max_length is not None:\n",
    "            Xt = Xt[:max_length]\n",
    "        Xs.append(Xt)\n",
    "        Ys.append(y)\n",
    "\n",
    "    # Compute lengths and target pad length\n",
    "    lengths = torch.tensor([x.size(0) for x in Xs], dtype=torch.long)\n",
    "\n",
    "    # Optionally pad L up to a multiple (helps on some accelerators)\n",
    "    if pad_to_multiple_of is not None:\n",
    "        max_len = int(lengths.max().item())\n",
    "        if max_len % pad_to_multiple_of != 0:\n",
    "            max_len = ((max_len + pad_to_multiple_of - 1) // pad_to_multiple_of) * pad_to_multiple_of\n",
    "        # Truncate already done; pad_sequence will handle padding up to the longest in list,\n",
    "        # so we extend each shorter sequence with EMPTY rows to reach max_len.\n",
    "        # pad_sequence itself canâ€™t force a larger-than-maximum length, so we manually\n",
    "        # right-pad each X to max_len first.\n",
    "        K = Xs[0].size(1)\n",
    "        padded_list = []\n",
    "        for x in Xs:\n",
    "            if x.size(0) < max_len:\n",
    "                pad_rows = max_len - x.size(0)\n",
    "                pad_block = x.new_full((pad_rows, K), pad_value)\n",
    "                padded_list.append(torch.cat([x, pad_block], dim=0))\n",
    "            else:\n",
    "                padded_list.append(x)\n",
    "        inputs = torch.stack(padded_list, dim=0)  # [B, L, K]\n",
    "    else:\n",
    "        # Let pad_sequence expand to the within-batch max\n",
    "        inputs = pad_sequence([_to_tensor(x) for x in Xs], batch_first=True, padding_value=pad_value)\n",
    "\n",
    "    # Build attention mask from true lengths (before any manual right-padding)\n",
    "    L = inputs.size(1)\n",
    "    arange = torch.arange(L).unsqueeze(0)  # [1, L]\n",
    "    attention_mask = (arange < lengths.unsqueeze(1))\n",
    "    attention_mask = attention_mask.to(mask_dtype)\n",
    "\n",
    "    # Stack targets robustly (supports tensors, numbers, tuples, dicts, etc.)\n",
    "    targets = default_collate([_to_tensor(y) for y in Ys])\n",
    "\n",
    "    return {\n",
    "        \"inputs\": inputs,                   # [B, L, K], float\n",
    "        \"attention_mask\": attention_mask,   # [B, L], bool (or chosen dtype)\n",
    "        \"lengths\": lengths,                 # [B]\n",
    "        \"targets\": targets                  # e.g., [B, T] or [B] depending on your dataset\n",
    "    }\n",
    "\n",
    "collate_fn = partial(\n",
    "    collate_embeddings_with_targets,\n",
    "    max_length=256,\n",
    "    pad_to_multiple_of=8,\n",
    "    pad_value=0.0,\n",
    "    mask_dtype=torch.bool,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f41cea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# 1) Tiny BERT: single encoder layer, keep pooler\n",
    "cfg = BertConfig(\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=1,          # one attention block\n",
    "    num_attention_heads=12,       # must divide 768\n",
    "    intermediate_size=1536,       # smaller FFN (optional)\n",
    "    max_position_embeddings=2048, # adjust as needed\n",
    "    add_pooling_layer=True,       # enables CLS pooler (dense + tanh)\n",
    "    vocab_size=1                  # unused since we pass inputs_embeds\n",
    ")\n",
    "device = 'cuda'\n",
    "model = BertModel(cfg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14388ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "dataloader = data.DataLoader(ds, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    for batch in dataloader:\n",
    "        X = batch['inputs'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        y = batch['targets'].to(device)\n",
    "        preds = model(inputs_embeds=X, attention_mask=mask)\n",
    "\n",
    "        loss = -torch.cosine_similarity(preds['pooler_output'], y).sum()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_samples += y.shape[0]\n",
    "        step += 1\n",
    "\n",
    "        if step % 1 == 0:\n",
    "            avg_loss = total_loss / total_samples\n",
    "            print(f\"Step {step} loss: {avg_loss:.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    print(f\"Epoch {epoch+1} loss: {avg_loss:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
